{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "# データ ドリフトの監視\r\n",
        "\r\n",
        "時間が経つにつれて、特徴データの傾向の変化により、モデルの正確な予測の効果が低下する可能性があります。この現象は*データ ドリフト*と呼ばれ、必要に応じてモデルを再トレーニングできるように、機械学習ソリューションを監視して検出することが重要です。\r\n",
        "\r\n",
        "このラボでは、データセットのデータ ドリフト監視を構成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## 開始する前に\r\n",
        "\r\n",
        "このノートブックのコードを実行するには、最新バージョンの **azureml-sdk** および **azureml-widgets** パッケージに加えて、**azureml-datadrift** パッケージが必要です。次のセルを実行して、パッケージがインストールされていることを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show azureml-datadrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## ワークスペースに接続する\r\n",
        "\r\n",
        "必要な SDK パッケージがインストールされているため、ワークスペースに接続できます。\r\n",
        "\r\n",
        "> **注**: Azure サブスクリプションでまだ認証済みのセッションを確立していない場合は、リンクをクリックして認証コードを入力し、Azure にサインインして認証するよう指示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to work with', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## *ベースライン* データセットを作成する\r\n",
        "\r\n",
        "データ ドリフトのデータセットを監視するには、*ベースライン* データセット (通常、モデルのトレーニングに使用されるデータセット) を登録して、将来収集されるデータとの比較ポイントとして使用する必要があります。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore, Dataset\n",
    "\n",
    "\n",
    "# Upload the baseline data\n",
    "default_ds = ws.get_default_datastore()\n",
    "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'],\n",
    "                       target_path='diabetes-baseline',\n",
    "                       overwrite=True, \n",
    "                       show_progress=True)\n",
    "\n",
    "# Create and register the baseline dataset\n",
    "print('Registering baseline dataset...')\n",
    "baseline_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-baseline/*.csv'))\n",
    "baseline_data_set = baseline_data_set.register(workspace=ws, \n",
    "                           name='diabetes baseline',\n",
    "                           description='diabetes baseline data',\n",
    "                           tags = {'format':'CSV'},\n",
    "                           create_new_version=True)\n",
    "\n",
    "print('Baseline dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## *ターゲット* データセットを作成する\r\n",
        "\r\n",
        "時間の経過とともに、ベースライン トレーニング データと同じ特徴を持つ新しいデータを収集できます。この新しいデータをベースライン データと比較するには、データ ドリフトを分析する特徴を含むターゲット データセットと、新しいデータが最新であった時点を示すタイムスタンプ フィールドを定義する必要があります。これにより、一時的なサイクル間隔でのデータ ドリフトを測定します。タイムスタンプは、データセット自体のフィールド、またはデータの格納に使用されるフォルダーとファイル名パターンから派生したフィールドのいずれかです。たとえば、月のフォルダーを含む年のフォルダーと、その日のフォルダーを含むフォルダー階層に新しいデータを保存できます。または、次のようにファイル名に年、月、日をエンコードすることもできます。*data_2020-01-29.csv*。これは、次のコードで採用されているアプローチです"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "print('Generating simulated data...')\n",
    "\n",
    "# Load the smaller of the two data files\n",
    "data = pd.read_csv('data/diabetes2.csv')\n",
    "\n",
    "# We'll generate data for the past 6 weeks\n",
    "weeknos = reversed(range(6))\n",
    "\n",
    "file_paths = []\n",
    "for weekno in weeknos:\n",
    "    \n",
    "    # Get the date X weeks ago\n",
    "    data_date = dt.date.today() - dt.timedelta(weeks=weekno)\n",
    "    \n",
    "    # Modify data to ceate some drift\n",
    "    data['Pregnancies'] = data['Pregnancies'] + 1\n",
    "    data['Age'] = round(data['Age'] * 1.2).astype(int)\n",
    "    data['BMI'] = data['BMI'] * 1.1\n",
    "    \n",
    "    # Save the file with the date encoded in the filename\n",
    "    file_path = 'data/diabetes_{}.csv'.format(data_date.strftime(\"%Y-%m-%d\"))\n",
    "    data.to_csv(file_path)\n",
    "    file_paths.append(file_path)\n",
    "\n",
    "# Upload the files\n",
    "path_on_datastore = 'diabetes-target'\n",
    "default_ds.upload_files(files=file_paths,\n",
    "                       target_path=path_on_datastore,\n",
    "                       overwrite=True,\n",
    "                       show_progress=True)\n",
    "\n",
    "# Use the folder partition format to define a dataset with a 'date' timestamp column\n",
    "partition_format = path_on_datastore + '/diabetes_{date:yyyy-MM-dd}.csv'\n",
    "target_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, path_on_datastore + '/*.csv'),\n",
    "                                                       partition_format=partition_format)\n",
    "\n",
    "# Register the target dataset\n",
    "print('Registering target dataset...')\n",
    "target_data_set = target_data_set.with_timestamp_columns('date').register(workspace=ws,\n",
    "                                                                          name='diabetes target',\n",
    "                                                                          description='diabetes target data',\n",
    "                                                                          tags = {'format':'CSV'},\n",
    "                                                                          create_new_version=True)\n",
    "\n",
    "print('Target dataset registered!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## データ ドリフト モニターを作成する\r\n",
        "\r\n",
        "これで、糖尿病データのデータ ドリフト モニターを作成する準備が整いました。データ ドリフト モニターは、定期的またはオンデマンドで実行され、ベースライン データセットとターゲット データセットを比較し、時間の経過とともに新しいデータが追加されます。\r\n",
        "\r\n",
        "### コンピューティング ターゲットを作成する\r\n",
        "\r\n",
        "データ ドリフト モニターを実行するには、コンピューティング ターゲットが必要です。次のセルを実行して、コンピューティング クラスターを指定します (存在しない場合は作成されます)。\r\n",
        "\r\n",
        "> **重要**: 実行する前に、以下のコードで *your-compute-cluster* をコンピューティング クラスターの名前に変更してください。クラスター名は、長さが 2 〜 16 文字のグローバルに一意の名前である必要があります。英字、数字、- の文字が有効です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"your-compute-cluster\"\n",
    "\n",
    "try:\n",
    "    # Check for existing compute target\n",
    "    training_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    # If it doesn't already exist, create it\n",
    "    try:\n",
    "        compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS11_V2', max_nodes=2)\n",
    "        training_cluster = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "        training_cluster.wait_for_completion(show_output=True)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "> **注**: コンピューティング インスタンスとクラスターは、スタンダードの Azure 仮想マシンのイメージに基づいています。この演習では、コストとパフォーマンスの最適なバランスを実現するために、*Standard_DS11_v2* イメージが推薦されます。サブスクリプションにこのイメージを含まないクォータがある場合は、別のイメージを選択してください。 ただし、画像が大きいほどコストが高くなり、小さすぎるとタスクが完了できない場合があることに注意してください。Azure 管理者にクォータを拡張するように依頼していただくことも可能です。\r\n",
        "\r\n",
        "### データ ドリフト モニターを定義する\r\n",
        "\r\n",
        "**DataDriftDetector** クラスを使用して、データのデータ ドリフト モニターを定義する準備ができました。データ ドリフトを監視する機能、監視プロセスの実行に使用するコンピューティング ターゲットの名前、データの比較頻度、アラートがトリガーされるデータのドリフトしきい値、データ収集を可能にする待機時間 (時間単位) を指定できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.datadrift import DataDriftDetector\n",
    "\n",
    "# set up feature list\n",
    "features = ['Pregnancies', 'Age', 'BMI']\n",
    "\n",
    "# set up data drift detector\n",
    "monitor = DataDriftDetector.create_from_datasets(ws, 'mslearn-diabates-drift', baseline_data_set, target_data_set,\n",
    "                                                      compute_target=cluster_name, \n",
    "                                                      frequency='Week', \n",
    "                                                      feature_list=features, \n",
    "                                                      drift_threshold=.3, \n",
    "                                                      latency=24)\n",
    "monitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## データ ドリフト モニターのバックフィルを行う\r\n",
        "\r\n",
        "6 週間のシミュレートされた毎週のデータ収集を含むベースライン データセットとターゲット データセットがあります。これを使用してモニターをバックフィルして、元のベースラインとターゲット データの間のデータ ドリフトを分析できます。\r\n",
        "\r\n",
        "> **注** バックフィル分析を実行するには、コンピューティング ターゲットを起動する必要があるため、実行に時間がかかる場合があります。ウィジェットは常に更新されて状態が表示されない場合があるため、リンクをクリックして、Azure Machine Learning Studio で実験の状態を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "backfill = monitor.backfill(dt.datetime.now() - dt.timedelta(weeks=6), dt.datetime.now())\n",
    "\n",
    "RunDetails(backfill).show()\n",
    "backfill.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## データ ドリフトを分析する\r\n",
        "\r\n",
        "次のコードを使用して、バックフィル実行で収集された時点のデータ ドリフトを調べることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_metrics = backfill.get_metrics()\n",
    "for metric in drift_metrics:\n",
    "    print(metric, drift_metrics[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "次の手順に従って、[Azure Machine Learning Studio](https://ml.azure.com) でデータのドリフト メトリックを視覚化することもできます。\r\n",
        "\r\n",
        "1. **データセット** ページで、**データセット モニター** タブを表示します。\r\n",
        "2. 表示するデータ ドリフト モニターをクリックします。\r\n",
        "3. データ ドリフト メトリックを表示する日付範囲を選択します (列グラフに複数の週のデータが表示されない場合は、1 分ほど待ってから**更新**をクリックします)。\r\n",
        "4. 上部の**ドリフト概要**セクションのグラフを調べ、全体的なドリフトの大きさと特徴ごとのドリフトの寄与度を表示します。\r\n",
        "5. 下部の**特徴の詳細**セクションのグラフを確認すると、個々の特徴のさまざまなドリフトの尺度を確認できます。\r\n",
        "\r\n",
        "> **注**: データのドリフト メトリックの理解については、Azure Machine Learning のドキュメントの「[データセットを監視する方法](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets#understanding-data-drift-results)」を参照してください。\r\n",
        "\r\n",
        "## さらに詳しく見る\r\n",
        "\r\n",
        "このラボは、データ ドリフト監視の概念と原則を紹介することを目的としています。データセットを使用したデータ ドリフトの監視の詳細については、Azure Machine Learning のドキュメントの[データセット上のデータ ドリフトの検出](https://docs.microsoft.com/azure/machine-learning/how-to-monitor-datasets)を参照してください。\r\n",
        "\r\n",
        "公開されたサービスからデータを収集し、それをデータドリフト監視のターゲット データセットとして使用することもできます。詳細については、[実稼働環境のモデルからデータを収集する](https://docs.microsoft.com/azure/machine-learning/how-to-enable-data-collection) を参照してください。\r\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}