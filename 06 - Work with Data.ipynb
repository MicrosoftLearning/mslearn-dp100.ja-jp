{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "# データを操作する\r\n",
        "\r\n",
        "データは、機械学習モデルが構築される基盤です。クラウドで一元的にデータを管理し、複数のワークステーションとコンピューティング ターゲットで実験を実行してモデルをトレーニングしているデータ サイエンティストのチームがアクセスできるようにすることは、プロフェッショナルなデータ サイエンス ソリューションでは重要です。\r\n",
        "\r\n",
        "このノートブックでは、データを操作するための 2 つの Azure Machine Learning オブジェクト、*データストア*と*データセット*について学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## ワークスペースに接続する\r\n",
        "\r\n",
        "作業を開始するには、ワークスペースに接続します。\r\n",
        "\r\n",
        "> **注**: Azure サブスクリプションでまだ認証済みのセッションを確立していない場合は、リンクをクリックして認証コードを入力し、Azure にサインインして認証するよう指示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "# Load the workspace from the saved config file\n",
    "ws = Workspace.from_config()\n",
    "print('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## データストアを操作する\r\n",
        "\r\n",
        "Azure ML では、*データストア*は、Azure Storage Blob コンテナーのようなストレージ場所への参照です。あらゆるワークスペースには既定のデータストアがあります。通常は、ワークスペースで作成された Azure Storage Blob コンテナーです。異なる場所に格納されているデータを使用する必要がある場合は、ワークスペースにカスタム データストアを追加して、既定に設定することができます。\r\n",
        "\r\n",
        "### データストアを表示する\r\n",
        "\r\n",
        "次のコードを実行してワークスペースでデータストアを判定します:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "また、[Azure Machine Learning Studio](https://ml.azure.com) のワークスペースに関する「**データストア**」ページでワークスペースのデータストアを表示して管理することもできます。\r\n",
        "\r\n",
        "### データストアにデータをアップロードする\r\n",
        "\r\n",
        "利用可能なデータストアを特定したので、実際に実験スクリプトが実行されている場所に関係なく、ワークスペースで実行中の実験にアクセスできるように、ローカル ファイル システムからファイルをアップロードできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_ds.upload_files(files=['./data/diabetes.csv', './data/diabetes2.csv'], # Upload the diabetes csv files in /data\n",
    "                       target_path='diabetes-data/', # Put it in a folder path in the datastore\n",
    "                       overwrite=True, # Replace existing files of the same name\n",
    "                       show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "## データセットを操作する\r\n",
        "\r\n",
        "Azure Machine Learning は*データセット*というかたちでデータの抽象化を提供します。データセットは、実験で使用したい特定のデータのセットへの参照で、バージョン管理されています。データセットは*表形式*または*ファイル*ベースのいずれかになります。\r\n",
        "\r\n",
        "### 表形式データセットを作成する\r\n",
        "\r\n",
        "データストアにアップロードした糖尿病データからデータセットを作成し、最初の 20 件のレコードを表示してみましょう。この場合、データは CSV ファイル内の構造化された形式なので、*表形式*のデータセットを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "#Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "tab_data_set = Dataset.Tabular.from_delimited_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "\n",
    "# Display the first 20 rows as a Pandas dataframe\n",
    "tab_data_set.take(20).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "上のコードでわかるように、表形式のデータセットを Pandas データフレームに変換するのは簡単で、一般的な Python の手法を使用してデータを操作できます。\r\n",
        "\r\n",
        "### ファイル データセットを作成する\r\n",
        "\r\n",
        "作成したデータセットは、データセット定義に含まれる構造化ファイル内のすべてのデータを含むデータフレームとして読み取ることができる*表形式*のデータセットです。これは表形式のデータに適していますが、機械学習のシナリオによっては、非構造化データの操作が必要となる場合があります。または、単に自分のコード内のファイルからデータの読み取り処理を行うことが必要となる場合もあります。これを実現するには、*ファイル* データセットを使用して、ファイルのデータを読み取るために使用できる仮想マウント ポイント内のファイル パスのリストを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a file dataset from the path on the datastore (this may take a short while)\n",
    "file_data_set = Dataset.File.from_files(path=(default_ds, 'diabetes-data/*.csv'))\n",
    "\n",
    "# Get the files in the dataset\n",
    "for file_path in file_data_set.to_path():\n",
    "    print(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "### データセットを登録する\r\n",
        "\r\n",
        "これで、糖尿病データを参照するデータセットを作成したので、それらを登録して、ワークスペースで実行されている実験に簡単にアクセスできるようにすることができます。\r\n",
        "\r\n",
        "表形式のデータセットを**糖尿病データセット**、ファイル データセットを**糖尿病ファイル**として登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the tabular dataset\n",
    "try:\n",
    "    tab_data_set = tab_data_set.register(workspace=ws, \n",
    "                                        name='diabetes dataset',\n",
    "                                        description='diabetes data',\n",
    "                                        tags = {'format':'CSV'},\n",
    "                                        create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "# Register the file dataset\n",
    "try:\n",
    "    file_data_set = file_data_set.register(workspace=ws,\n",
    "                                            name='diabetes file dataset',\n",
    "                                            description='diabetes files',\n",
    "                                            tags = {'format':'CSV'},\n",
    "                                            create_new_version=True)\n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "\n",
    "print('Datasets registered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "[Azure Machine Learning Studio](https://ml.azure.com) のワークスペースに関する「**データセット**」ページでデータセットを表示して管理できます。ワークスペース オブジェクトからもデータセットのリストを取得できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Datasets:\")\n",
    "for dataset_name in list(ws.datasets.keys()):\n",
    "    dataset = Dataset.get_by_name(ws, dataset_name)\n",
    "    print(\"\\t\", dataset.name, 'version', dataset.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "データセットをバージョン管理できるため、以前の定義に依存する既存の実験やパイプラインを壊すことなくデータセットを再定義できます。既定では、名前付きデータセットの最新バージョンが返されますが、次のようにバージョン番号を指定することで、特定のバージョンのデータセットを取得できます。\r\n",
        "\r\n",
        "```python\r\n",
        "dataset_v1 = Dataset.get_by_name(ws, 'diabetes dataset', version = 1)\r\n",
        "```\r\n",
        "\r\n",
        "\r\n",
        "### 表形式データセットからモデルをトレーニングする\r\n",
        "\r\n",
        "データセットができたので、そこからモデルのトレーニングを開始する準備が整いました。データセットは、スクリプトの実行に使用される Estimator で、*入力*としてスクリプトに渡すことができます。\r\n",
        "\r\n",
        "次の 2 つのコード セルを実行して作成します。\r\n",
        "\r\n",
        "1. **diabetes_training_from_tab_dataset** という名前のフォルダー\r\n",
        "2. 引数として渡される表形式のデータセットを使用して分類モデルをトレーニングするスクリプト。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = 'diabetes_training_from_tab_dataset'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "from azureml.core import Run, Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Get the script arguments (regularization rate and training dataset ID)\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
    "parser.add_argument(\"--input-data\", type=str, dest='training_dataset_id', help='training dataset')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set regularization hyperparameter (passed as an argument to the script)\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# Get the training dataset\n",
    "print(\"Loading Data...\")\n",
    "diabetes = run.input_datasets['training_data'].to_pandas_dataframe()\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "> **注**: スクリプトでは、データセットはパラメーター (または引数) として渡されます。表形式のデータセットの場合、この引数には登録済みのデータセットの ID が含まれます。このため、スクリプトにコードを書き込んで実験のワークスペースを実行コンテキストから取得した後、以下のような ID を利用してデータセットを取得できます。\r\n",
        ">\r\n",
        "> ```\r\n",
        "> run = Run.get_context()\r\n",
        "> ws = run.experiment.workspace\r\n",
        "> dataset = Dataset.get_by_id(ws, id=args.training_dataset_id)\r\n",
        "> diabetes = dataset.to_pandas_dataframe()\r\n",
        "> ```\r\n",
        ">\r\n",
        "> ただし、Azure Machine Learning 実行では、名前の付いたデータセットを参照して、これを実験の **input_datasets** コレクションに追加する引数が自動的に識別されます。このため、「フレンドリ名」を指定して、このコレクションからデータセットを取得することもできます (後述しますが、この名前は実験のスクリプト実行構成の引数定義で指定されます)。このアプローチは上記のスクリプトで使用されています。\r\n",
        "\r\n",
        "これでスクリプトを実験として実行し、スクリプトで読み取られるトレーニング データセットの引数を定義できます。\r\n",
        "\r\n",
        "> **注**: **Dataset** クラスは、**azureml-dataprep** パッケージの一部のコンポーネントに依存しているため、トレーニング実験の環境にこのパッケージを含める必要があります。**azureml-dataprep** パッケージは、**azure-defaults** パッケージに含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment, ScriptRunConfig, Environment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "# Create a Python environment for the experiment (from a .yml file)\n",
    "env = Environment.from_conda_specification(\"experiment_env\", \"environment.yml\")\n",
    "\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"diabetes dataset\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                              script='diabetes_training.py',\n",
    "                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n",
    "                              environment=env) \n",
    "\n",
    "# submit the experiment\n",
    "experiment_name = 'mslearn-train-diabetes'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "> **注:** **--Input-data** 引数は*名前の付いた入力*としてデータセットを渡します。この中には、スクリプトが実験実行の **input_datasets** コレクションから読み取るために使用する、データセットの*フレンドリ名*が含まれます。**--Input-data** 引数の文字列値は実際には登録済みのデータセットの ID です。  代替アプローチとして、`diabetes_ds.id` を渡すこともできます。この場合、スクリプトはスクリプト引数からデータセット ID にアクセスし、これを使用してデータセットをワークスペースから取得できますが、**input_datasets** コレクションからは取得できません。\r\n",
        "\r\n",
        "初めて実験を実行すると、Python 環境のセットアップに時間がかかる場合があります。以降の実行はより高速になります。\r\n",
        "\r\n",
        "実験が完了したら、ウィジェットで、**azureml-logs/70_driver_log.txt** 出力ログと実行によって生成されたメトリックを表示します。\r\n",
        "\r\n",
        "### トレーニングされたモデルを登録する\r\n",
        "\r\n",
        "他のトレーニング実験と同様、Azure Machine Learning ワークスペースではトレーニングされたモデルを取得して登録することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
    "                   tags={'Training context':'Tabular dataset'}, properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "### ファイル データセットからモデルをトレーニングする\r\n",
        "\r\n",
        "*表形式*データセットでトレーニング データを使用してモデルをトレーニングする方法を見てきましたが、*ファイル* データセットについてはどうでしょうか。\r\n",
        "\r\n",
        "ファイル データセットを使用する場合、スクリプトに渡されるデータセット引数は、ファイル パスを含むマウント ポイントを表します。これらのファイルからデータを読み取る方法は、ファイル内のデータの種類と、そのデータを使用して何を行うかによって異なります。糖尿病 CSV ファイルの場合、Python **glob** モジュールを使用して、データセットによって定義された仮想マウント ポイント内のファイルのリストを作成し、それらをすべて単一のデータフレームに連結された Pandas データフレームに読み込むことができます。\r\n",
        "\r\n",
        "次の 2 つのコード セルを実行して作成します。\r\n",
        "\r\n",
        "1. **diabetes_training_from_file_dataset** という名前のフォルダー\r\n",
        "2. *入力*として渡されるファイル データセットを使用して分類モデルをトレーニングするスクリプト。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a folder for the experiment files\n",
    "experiment_folder = 'diabetes_training_from_file_dataset'\n",
    "os.makedirs(experiment_folder, exist_ok=True)\n",
    "print(experiment_folder, 'folder created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $experiment_folder/diabetes_training.py\n",
    "# Import libraries\n",
    "import os\n",
    "import argparse\n",
    "from azureml.core import Dataset, Run\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import glob\n",
    "\n",
    "# Get script arguments (rgularization rate and file dataset mount point)\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--regularization', type=float, dest='reg_rate', default=0.01, help='regularization rate')\n",
    "parser.add_argument('--input-data', type=str, dest='dataset_folder', help='data mount point')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Set regularization hyperparameter (passed as an argument to the script)\n",
    "reg = args.reg_rate\n",
    "\n",
    "# Get the experiment run context\n",
    "run = Run.get_context()\n",
    "\n",
    "# load the diabetes dataset\n",
    "print(\"Loading Data...\")\n",
    "data_path = run.input_datasets['training_files'] # Get the training data path from the input\n",
    "# (You could also just use args.dataset_folder if you don't want to rely on a hard-coded friendly name)\n",
    "\n",
    "# Read the files\n",
    "all_files = glob.glob(data_path + \"/*.csv\")\n",
    "diabetes = pd.concat((pd.read_csv(f) for f in all_files), sort=False)\n",
    "\n",
    "# Separate features and labels\n",
    "X, y = diabetes[['Pregnancies','PlasmaGlucose','DiastolicBloodPressure','TricepsThickness','SerumInsulin','BMI','DiabetesPedigree','Age']].values, diabetes['Diabetic'].values\n",
    "\n",
    "# Split data into training set and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n",
    "\n",
    "# Train a logistic regression model\n",
    "print('Training a logistic regression model with regularization rate of', reg)\n",
    "run.log('Regularization Rate',  np.float(reg))\n",
    "model = LogisticRegression(C=1/reg, solver=\"liblinear\").fit(X_train, y_train)\n",
    "\n",
    "# calculate accuracy\n",
    "y_hat = model.predict(X_test)\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy:', acc)\n",
    "run.log('Accuracy', np.float(acc))\n",
    "\n",
    "# calculate AUC\n",
    "y_scores = model.predict_proba(X_test)\n",
    "auc = roc_auc_score(y_test,y_scores[:,1])\n",
    "print('AUC: ' + str(auc))\n",
    "run.log('AUC', np.float(auc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=model, filename='outputs/diabetes_model.pkl')\n",
    "\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "表形式のデータセットと同様、ファイル データセットもフレンドリ名を使用して **input_datasets** コレクションから取得できます。また、スクリプト引数から取得することも可能です。この場合、ファイル データセットにはファイルへのマウント パスが含まれます (表形式のデータセットで渡されるデータセット ID とは異なります)。\r\n",
        "\r\n",
        "次に、データセットをスクリプトに渡す方法を変更する必要があります。スクリプトがファイルを読み取るパスを定義しなくてはなりません。これを行うには、**as_download** または **as_mount** メソッドのいずれかを使用できます。**as_download** を使用すると、ファイル データセットのファイルは、スクリプトを実行しているコンピューティングの一時的な場所にダウンロードされます。一方、**as_mount** は、ファイルを直接、データストアからストリームできるマウント ポイントを作成します。\r\n",
        "\r\n",
        "アクセス メソッドを **as_named_input** メソッドと組み合わせ、実験実行で **input_datasets** コレクションにデータセットを含めることができます (引数を `diabetes_ds.as_mount()` に設定するなどしてこれを省略すると、スクリプトはスクリプト引数からデータセット マウント ポイントにアクセスできますが、**input_datasets** コレクションからはアクセスできません)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "# Get the training dataset\n",
    "diabetes_ds = ws.datasets.get(\"diabetes file dataset\")\n",
    "\n",
    "# Create a script config\n",
    "script_config = ScriptRunConfig(source_directory=experiment_folder,\n",
    "                                script='diabetes_training.py',\n",
    "                                arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n",
    "                                             '--input-data', diabetes_ds.as_named_input('training_files').as_download()], # Reference to dataset location\n",
    "                                environment=env) # Use the environment created previously\n",
    "\n",
    "# submit the experiment\n",
    "experiment_name = 'mslearn-train-diabetes'\n",
    "experiment = Experiment(workspace=ws, name=experiment_name)\n",
    "run = experiment.submit(config=script_config)\n",
    "RunDetails(run).show()\n",
    "run.wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "実験が完了したら、ウィジェットで **azureml-logs/70_driver_log.txt** 出力ログを表示し、ファイル データセットのファイルが一時的なフォルダーにダウンロードされており、スクリプトがファイルを読み取れることを確認します。\r\n",
        "\r\n",
        "### トレーニングされたモデルを登録する\r\n",
        "\r\n",
        "もう一度、実験によってトレーニングされたモデルを登録できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "run.register_model(model_path='outputs/diabetes_model.pkl', model_name='diabetes_model',\n",
    "                   tags={'Training context':'File dataset'}, properties={'AUC': run.get_metrics()['AUC'], 'Accuracy': run.get_metrics()['Accuracy']})\n",
    "\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version:', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
        "> **詳細情報**: データセットを使用するトレーニングの詳細については、Azure ML ドキュメントの[データセットを使用するトレーニング](https://docs.microsoft.com/azure/machine-learning/how-to-train-with-datasets)を参照してください。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}